---
title: "Concrete: Surface Crack Detection"
date: 2020-04-11
tags: [data science, image processing, machine learning, deep learning, supervised learning, classification] 
header:
excerpt: "Classification"
mathjax: true
---

## Feedback on JavaPresse Coffee Grinder

<font size="5"><h3>Background</h3></font>

<font size="4"><div style="text-align: justify"><p>Concrete surface cracks are major defect in civil structures. Crack detection plays a major role in building inspection, finding cracks and determining building health.</p>
  
<font size="4"><div style="text-align: justify"><p>Developing a system to automatically classify instances would be beneficial especially when size of the dataset starts getting larger.</p>

<font size="5"><h3>Data Description</h3></font>

<font size="4"><p>The data to be analyzed includes</p>

<ul>
  <li>20000 images with cracks;</li>
  <li>20000 images without cracks.</li>
</ul>

<div style="text-align: justify"><p>The dataset used for the project is available on Kaggle and can be accessed using <a href="https://www.kaggle.com/arunrk7/surface-crack-detection">this</a> link.</p>

<font size="5"><h3>Methodology</h3></font>

<p></p>

<div style="text-align: justify"><p>Image paths were scored into separate dataframes for training, validation and testing to make them easier to preprocess and evaluate.</p>

<div style="text-align: justify"><p>The model used was defined by importing the network (with pre-trained weights) developed by the Visual Geometry Group at Oxford which won the ImageNet Large Scale Visual Recognition Challenge in 2014, chopping off the top layer and freezing other leayers before finally being connected to a global averaging layer followed by two dense layers that were finally fed into a sigmoid activation for classification.</p>
  
<div style="text-align: justify"><p>Training, validation and testing steps were defined as the result of the division between the number of samples in each set and the corresponding batch sizes. Augmentation was applied to the training set to prevent overfitting and the images were resized appropriately.</p>
  
<div style="text-align: justify"><p> Working with a large number of samples simultaneously and performing numeric computations on them will overstrain the Random Access Memory and can result in the kernel crashing which will also be prevented by working with data in smaller batches.</p>
  
<font size="5"><h3>Results</h3></font>

<div style="text-align: justify"><p>The neural network model was trained and validated on the data for a single epoch which was expedited using the Graphics Processing Unit on Kaggle to produce the results shown below.</p>

<div style="text-align: center"><img src="{{ site.url }}{{ site.baseurl }}/assets/images/cscd/cscd_1.jpg">
  
<p></p>
  
<div style="text-align: justify"><p>A copy of the working notebook is available <a href="https://github.com/gopalrahulrg/gopalrahulrg.github.io/blob/master/assets/books/cscd/rg-cscd_11_04_2020.ipynb">here</a>; the setup can also run directly on Kaggle after being forked from <a href="https://www.kaggle.com/gopalrahulrg/oxfordnet-flow-batch-processing">this</a> link.</p>
  
<p></p>

<font size="5"><h3>Conclusion</h3></font>

<div style="text-align: justify"><p>The accuracy on the test set was over 99.50 % without any fine tuning which is excellent.</p>
