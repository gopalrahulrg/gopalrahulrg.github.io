{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Updated: Apr 11, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a quick overview of how to apply transfer learning and flow batch processing of \"relatively large\" datasets for neural networks and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would also like to thank the IBM teaching staff on Coursera for their support on applying deep learning using neural networks for image processing and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# importing library to handle files\n",
    "import os\n",
    "\n",
    "# importing libray to handle status bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# import libray to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# importing library to process data structures\n",
    "import pandas as pd\n",
    "\n",
    "# importing library to deal with numeric arrays\n",
    "import numpy as np\n",
    "\n",
    "# importing deep learning library\n",
    "import tensorflow as tf\n",
    "\n",
    "# importing library for preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3d8c48e61d4ea896ac046d341af6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initializing lists to store file paths for training and validation\n",
    "img_paths = []\n",
    "\n",
    "# importing libraries to store label references\n",
    "labels = []\n",
    "\n",
    "# iterating through directories\n",
    "for dirname, _, filenames in tqdm(os.walk('/kaggle/input')):\n",
    "    for filename in filenames:\n",
    "        \n",
    "        path = os.path.join(dirname, filename)\n",
    "        \n",
    "        if '.jpg' in path:\n",
    "        \n",
    "            img_paths.append(path)\n",
    "            labels.append(path.split(os.path.sep)[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes for training, validation and test datasets\n",
    "main_df = pd.DataFrame({'Path': img_paths, 'Label': labels}).sample(frac = 1,\n",
    "                                                                    random_state = 10)\n",
    "\n",
    "oX_train, X_test, oy_train, y_test = train_test_split(main_df['Path'], main_df['Label'], test_size = 0.2,\n",
    "                                                      stratify = main_df['Label'], \n",
    "                                                      shuffle = True, random_state = 20)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(oX_train, oy_train, test_size = 0.2,\n",
    "                                                  stratify = oy_train, \n",
    "                                                  shuffle = True, random_state = 40)\n",
    "\n",
    "# train dataframe\n",
    "train_df = pd.DataFrame({'Path': X_train, 'Label': y_train})\n",
    "\n",
    "# validation dataframe\n",
    "val_df = pd.DataFrame({'Path': X_val, 'Label': y_val})\n",
    "\n",
    "# test dataframe\n",
    "test_df = pd.DataFrame({'Path': X_test, 'Label': y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "Found 25600 validated image filenames belonging to 2 classes.\n",
      "Found 6400 validated image filenames belonging to 2 classes.\n",
      "Found 8000 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# setting image dimensions\n",
    "IMAGE_DIMS = (224, 224, 3)\n",
    "\n",
    "# loading preprocessing function\n",
    "prep_func = tf.keras.applications.vgg16.preprocess_input \n",
    "        \n",
    "# importing pretrained model\n",
    "vgg_model = tf.keras.applications.vgg16.VGG16(input_shape = IMAGE_DIMS,\n",
    "                                              include_top = False, weights = 'imagenet')\n",
    "        \n",
    "# freezing layers in pretrained model\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# training generator for augmentation\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function = prep_func,\n",
    "                                                                rotation_range = 10,  \n",
    "                                                                zoom_range = 0.1, width_shift_range = 0.1,  \n",
    "                                                                height_shift_range = 0.1, \n",
    "                                                                horizontal_flip = True, \n",
    "                                                                vertical_flip = True)  \n",
    "\n",
    "# validation/testing generator for augmentation\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function = prep_func)\n",
    "\n",
    "# batch size for training\n",
    "train_bs = 256\n",
    "\n",
    "# loading training data in batches\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe = train_df, x_col = \"Path\",\n",
    "                                                    y_col = \"Label\", target_size = (IMAGE_DIMS[1], \n",
    "                                                                                    IMAGE_DIMS[0]),\n",
    "                                                    batch_size = train_bs, \n",
    "                                                    class_mode = 'binary')\n",
    "\n",
    "# batch size for validation\n",
    "val_bs = 128\n",
    "\n",
    "# loading validation data in batches\n",
    "val_generator = val_datagen.flow_from_dataframe(dataframe = val_df, x_col=\"Path\",\n",
    "                                                y_col = \"Label\", target_size = (IMAGE_DIMS[1], \n",
    "                                                                                IMAGE_DIMS[0]),\n",
    "                                                batch_size = val_bs, \n",
    "                                                class_mode = 'binary',\n",
    "                                                shuffle = False)\n",
    "\n",
    "# batch size for testing\n",
    "test_bs = 160\n",
    "    \n",
    "# loading test data in batches\n",
    "test_generator = val_datagen.flow_from_dataframe(dataframe = test_df, x_col = \"Path\",\n",
    "                                                 y_col = \"Label\", target_size = (IMAGE_DIMS[1], \n",
    "                                                                                 IMAGE_DIMS[0]),\n",
    "                                                 batch_size = test_bs, \n",
    "                                                 class_mode = 'binary',\n",
    "                                                 shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 15,240,513\n",
      "Trainable params: 525,825\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining a sequential model to learn \n",
    "clf_model = tf.keras.Sequential()\n",
    "\n",
    "# adding pretrained model\n",
    "clf_model.add(vgg_model)\n",
    "\n",
    "# using global average pooling instead of flatten and global max pooling\n",
    "clf_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "clf_model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "clf_model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "clf_model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "clf_model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "clf_model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "# model summary\n",
    "clf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps for training: 100.0, validation: 50.0, testing: 50.0\n"
     ]
    }
   ],
   "source": [
    "# calculating steps for train, validation and testing\n",
    "steps_train = np.ceil(train_df.shape[0]/train_bs)\n",
    "steps_val = np.ceil(val_df.shape[0]/val_bs)\n",
    "steps_test = np.ceil(test_df.shape[0]/test_bs)\n",
    "\n",
    "print(\"Steps for training:\", str(steps_train) + ',', \"validation:\", str(steps_val) + ',', \n",
    "      \"testing:\", str(steps_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100.0 steps, validate for 50.0 steps\n",
      "100/100 [==============================] - 383s 4s/step - loss: 0.0967 - accuracy: 0.9642 - val_loss: 0.0086 - val_accuracy: 0.9973\n"
     ]
    }
   ],
   "source": [
    "# compiling the model\n",
    "clf_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# training\n",
    "history = clf_model.fit_generator(train_generator, steps_per_epoch = steps_train,\n",
    "                                  validation_data = val_generator, epochs = 1,\n",
    "                                  validation_steps = steps_val, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 36s 724ms/step - loss: 0.0088 - accuracy: 0.9977\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "clf_eval = clf_model.evaluate_generator(test_generator, steps = steps_test, \n",
    "                                        verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test: 0.99775\n"
     ]
    }
   ],
   "source": [
    "# getting accuracy from evaluation\n",
    "print(\"Accuracy on Test:\", clf_eval[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1b3d8c48e61d4ea896ac046d341af6b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6a529efc99de45468b5db61c33f3c8cc",
        "IPY_MODEL_1f5163d1622b46c39562053824722ef8"
       ],
       "layout": "IPY_MODEL_a5d6d5b710b446e6860a36def77928a7"
      }
     },
     "1f5163d1622b46c39562053824722ef8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9d3d892408f94b12be270bb3822c1ab0",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_6e7f01456a164a38a08d8f6d02f78ac4",
       "value": " 4/? [00:07&lt;00:00,  1.97s/it]"
      }
     },
     "6a529efc99de45468b5db61c33f3c8cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_86dba52339cc4936a6e1c0845b8ee966",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e85b6f116edb407ca56324d1c7e696f4",
       "value": 1
      }
     },
     "6e7f01456a164a38a08d8f6d02f78ac4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "86dba52339cc4936a6e1c0845b8ee966": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d3d892408f94b12be270bb3822c1ab0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5d6d5b710b446e6860a36def77928a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e85b6f116edb407ca56324d1c7e696f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
